{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifier for Sonar Readings\n",
    "\n",
    "In this document I will be copying code from https://machinelearningmastery.com/building-a-binary-classification-model-in-pytorch/. This code uses PyTorch to design and train a neural network on training data. It will then evaluate the performance of the neural network using a k-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset describes data from a sonar chirp which returns bouncing off of different services. There are 60 input variables which each have the strength of the returns at different angles. The classification problem will determine whether it has bounced off of a rock or a metal cylinder\n",
    "\n",
    "First we need to import pandas so that we can read in the dataset and set the X as the independent variables and the Y as the label or dependent variable which in this case is whether it is a rock or a metal cylinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"sonar.csv\", header=None) #read file in\n",
    "X = data.iloc[:, 0:60] #all independent variables\n",
    "y = data.iloc[:, 60] #dependent variable or label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label which is in the y variable needs to be converted froma a string to a numeric label. As there is only 2 labels, 'M' and 'R' these can be converted to 1 and 0 respectively. \n",
    "\n",
    "This can be done using sklearn and the encoder function which will do this automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check this has been done correctly we can use encoder.classes_ to check the classes and we can also print y to see the data outputs. When using encoder.classes_ this should us 'M' and 'R'. When printing y, we should see 1's and 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'R']\n"
     ]
    }
   ],
   "source": [
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen this has worked and we have received our expected output. The 0 represents 'M' and the 1 represents the 'R'.\n",
    "\n",
    "Next we need to convert these in to PyTorch tensors so that we can feed it into our PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be creating a 3 layer neural network model that only has 1 hidden layer. As this model has 60 pieces of input data to predict one binary variable. As we want to make a wide model with one hidden layer, a hidden layer of 180 neurons would be a good model. 180 is a three times the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new class which creates a model with one hidden layer\n",
    "class Wide(nn.Module):\n",
    "    #initialiser\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #creates linear transformation from input layer to hidden layer\n",
    "        self.hidden = nn.Linear(60, 180)\n",
    "        self.relu = nn.ReLU()\n",
    "        #creates linear transformation from hidden layer to output layer\n",
    "        self.output = nn.Linear(180, 1)\n",
    "        #applies sigmoid function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    #method to move data from left layer to right layer, takes in the data as paramater\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are alos going to create a second model which uses 3 hidden layers. This is called a deeper model as it has more than one hidden layer. This model will have 3 layers each with 60 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(60, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(60, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the 2 models have similar number of parameters by running the following code. When this code is run we should get 2 numbers which are vaguely similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11161\n",
      "11041\n"
     ]
    }
   ],
   "source": [
    "# Compare model sizes\n",
    "model1 = Wide()\n",
    "model2 = Deep()\n",
    "print(sum([x.reshape(-1).shape[0] for x in model1.parameters()]))  # 11161\n",
    "print(sum([x.reshape(-1).shape[0] for x in model2.parameters()]))  # 11041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above cell the models have been created successfully as the two numbers are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop can be defined below. This function will be called for every batch that the stratified testing loop. In this case the function will be called 5 times as we have split our data into 5 batches. Below is the code. Our function takes in a model, our training datasets and our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "    n_epochs = 250   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    #loop through number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        #train the model\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both models have been created we now need ot train the data using our testing set. A testing method that we can use is called k-fold cross validation. This splits a large dataset into k amount of portions and takes one portion as the test set while the k-1 portions are the training set. There will be k number of combinations and the size of the training set will increase each time. \n",
    "\n",
    "Scikit-learn will use stratified k fold which means when seperating the data into the portions, it will ensure that there is a fair distribution of data in each portion.\n",
    "\n",
    "First we need to import the needed libraries from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (wide): 0.86\n",
      "Accuracy (wide): 0.76\n",
      "Accuracy (wide): 0.81\n",
      "Accuracy (wide): 0.63\n",
      "Accuracy (wide): 0.68\n",
      "Model accuracy: 74.91% (+/- 8.13%)\n"
     ]
    }
   ],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Wide()\n",
    "    acc = model_train(model, X[train], y[train], X[test], y[test])\n",
    "    print(\"Accuracy (wide): %.2f\" % acc)\n",
    "    cv_scores.append(acc)\n",
    "\n",
    "# evaluate the model\n",
    "acc = np.mean(cv_scores)\n",
    "std = np.std(cv_scores)\n",
    "print(\"Model accuracy: %.2f%% (+/- %.2f%%)\" % (acc*100, std*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code creates a training dataset and a testing dataset by splitting the dataset into a 70/30 split. The data is also shuffled before it is split to ensure that the data is unabiased between the training and test set. The same stratified training is used as above and is done for both the wide and deep model. Both will print out the accuracy after each batch has been passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (wide): 0.76\n",
      "Accuracy (wide): 0.76\n",
      "Accuracy (wide): 0.90\n",
      "Accuracy (wide): 0.79\n",
      "Accuracy (wide): 0.66\n",
      "Accuracy (deep): 0.90\n",
      "Accuracy (deep): 0.90\n",
      "Accuracy (deep): 0.79\n",
      "Accuracy (deep): 0.79\n",
      "Accuracy (deep): 0.76\n",
      "Wide: 77.24% (+/- 7.74%)\n",
      "Deep: 82.76% (+/- 5.77%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_wide = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Wide()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (wide): %.2f\" % acc)\n",
    "    cv_scores_wide.append(acc)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Deep()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    "\n",
    "# evaluate the model\n",
    "wide_acc = np.mean(cv_scores_wide)\n",
    "wide_std = np.std(cv_scores_wide)\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on which model is more accurate, the more accurate model will be trained once again with the entire training data set. The model accuracy will be outputted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain a deep model\n",
      "Final model accuracy: 82.54%\n"
     ]
    }
   ],
   "source": [
    "# rebuild model with full set of training data\n",
    "if wide_acc > deep_acc:\n",
    "    print(\"Retrain a wide model\")\n",
    "    model = Wide()\n",
    "else:\n",
    "    print(\"Retrain a deep model\")\n",
    "    model = Deep()\n",
    "acc = model_train(model, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the final output layer and what the expected output should be. We are looking for the first number to be as close to the expected number as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0132 0.008  0.0188 0.0141 0.0436 0.0668 0.0609 0.0131 0.0899 0.0922\n",
      " 0.1445 0.1475 0.2087 0.2558 0.2603 0.1985 0.2394 0.3134 0.4077 0.4529\n",
      " 0.4893 0.5666 0.6234 0.6741 0.8282 0.8823 0.9196 0.8965 0.7549 0.6736\n",
      " 0.6463 0.5007 0.3663 0.2298 0.1362 0.2123 0.2395 0.2673 0.2865 0.206\n",
      " 0.1659 0.2633 0.2552 0.1696 0.1467 0.1286 0.0926 0.0716 0.0325 0.0258\n",
      " 0.0136 0.0044 0.0028 0.0021 0.0022 0.0048 0.0138 0.014  0.0028 0.0064] -> [0.5399592] (expected [1.])\n",
      "[0.0274 0.0242 0.0621 0.056  0.1129 0.0973 0.1823 0.1745 0.144  0.1808\n",
      " 0.2366 0.0906 0.1749 0.4012 0.5187 0.7312 0.9062 0.926  0.7434 0.4463\n",
      " 0.5103 0.6952 0.7755 0.8364 0.7283 0.6399 0.5759 0.4146 0.3495 0.4437\n",
      " 0.2665 0.2024 0.1942 0.0765 0.3725 0.5843 0.4827 0.2347 0.0999 0.3244\n",
      " 0.399  0.2975 0.1684 0.1761 0.1683 0.0729 0.119  0.1297 0.0748 0.0067\n",
      " 0.0255 0.0113 0.0108 0.0085 0.0047 0.0074 0.0104 0.0161 0.022  0.0173] -> [0.93508375] (expected [1.])\n",
      "[0.0303 0.0353 0.049  0.0608 0.0167 0.1354 0.1465 0.1123 0.1945 0.2354\n",
      " 0.2898 0.2812 0.1578 0.0273 0.0673 0.1444 0.207  0.2645 0.2828 0.4293\n",
      " 0.5685 0.699  0.7246 0.7622 0.9242 1.     0.9979 0.8297 0.7032 0.7141\n",
      " 0.6893 0.4961 0.2584 0.0969 0.0776 0.0364 0.1572 0.1823 0.1349 0.0849\n",
      " 0.0492 0.1367 0.1552 0.1548 0.1319 0.0985 0.1258 0.0954 0.0489 0.0241\n",
      " 0.0042 0.0086 0.0046 0.0126 0.0036 0.0035 0.0034 0.0079 0.0036 0.0048] -> [0.04456715] (expected [0.])\n",
      "[0.0125 0.0152 0.0218 0.0175 0.0362 0.0696 0.0873 0.0616 0.1252 0.1302\n",
      " 0.0888 0.05   0.0628 0.1274 0.0801 0.0742 0.2048 0.295  0.3193 0.4567\n",
      " 0.5959 0.7101 0.8225 0.8425 0.9065 0.9802 1.     0.8752 0.7583 0.6616\n",
      " 0.5786 0.5128 0.4776 0.4994 0.5197 0.5071 0.4577 0.3505 0.1845 0.189\n",
      " 0.1967 0.1041 0.055  0.0492 0.0622 0.0505 0.0247 0.0219 0.0102 0.0047\n",
      " 0.0019 0.0041 0.0074 0.003  0.005  0.0048 0.0017 0.0041 0.0086 0.0058] -> [0.9727021] (expected [1.])\n",
      "[0.0323 0.0101 0.0298 0.0564 0.076  0.0958 0.099  0.1018 0.103  0.2154\n",
      " 0.3085 0.3425 0.299  0.1402 0.1235 0.1534 0.1901 0.2429 0.212  0.2395\n",
      " 0.3272 0.5949 0.8302 0.9045 0.9888 0.9912 0.9448 1.     0.9092 0.7412\n",
      " 0.7691 0.7117 0.5304 0.2131 0.0928 0.1297 0.1159 0.1226 0.1768 0.0345\n",
      " 0.1562 0.0824 0.1149 0.1694 0.0954 0.008  0.079  0.1255 0.0647 0.0179\n",
      " 0.0051 0.0061 0.0093 0.0135 0.0063 0.0063 0.0034 0.0032 0.0062 0.0067] -> [0.06146875] (expected [0.])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
